# -*- coding: utf-8 -*-
# === AI SIGNATURE ===
# Created by: Claude (opus-4.5)
# Created at: 2026-01-30T02:15:00Z
# Purpose: AI Supervisor - Watchdog daemon for Production Engine
# Contract: Monitor â†’ Detect crash â†’ Auto-restart â†’ Alert
# === END SIGNATURE ===
"""
HOPE AI SUPERVISOR - Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ´ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‰Ğ¸Ğº Ğ·Ğ° Production Engine

Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸:
  1. ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ heartbeat Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°
  2. ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¸
  3. Exponential backoff Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸ÑÑ…
  4. Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ…
  5. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ (crash forensics)
  6. Graceful shutdown Ğ¿Ğ¾ STOP.flag Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñƒ

ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AI SUPERVISOR  â”‚
  â”‚  (this script)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ monitors
           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   HEARTBEAT     â”‚â”€â”€â”€â”€â–¶â”‚  PRODUCTION     â”‚
  â”‚   state/ai/     â”‚     â”‚  ENGINE         â”‚
  â”‚   production/   â”‚     â”‚                 â”‚
  â”‚   heartbeat.jsonâ”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼ if stale
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AUTO-RESTART   â”‚
  â”‚  + TELEGRAM     â”‚
  â”‚  ALERT          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usage:
    python scripts/hope_supervisor.py [--mode TESTNET] [--interval 30]
    python scripts/hope_supervisor.py --status
    python scripts/hope_supervisor.py --stop
"""

import argparse
import asyncio
import json
import logging
import os
import signal
import subprocess
import sys
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional, Dict, List

sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger("SUPERVISOR")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Paths
ROOT = Path(__file__).parent.parent
STATE_DIR = ROOT / "state" / "ai" / "production"
LOCKS_DIR = ROOT / "state" / "locks"
LOGS_DIR = ROOT / "logs"

HEARTBEAT_FILE = STATE_DIR / "heartbeat.json"
ENGINE_LOCK_FILE = LOCKS_DIR / "production_engine.lock"
SUPERVISOR_LOCK_FILE = LOCKS_DIR / "supervisor.lock"
SUPERVISOR_STATE_FILE = STATE_DIR / "supervisor_state.json"
STOP_FLAG_FILE = ROOT / "state" / "STOP.flag"
CRASH_LOG_FILE = STATE_DIR / "crash_log.jsonl"

# Timing
DEFAULT_HEARTBEAT_TIMEOUT = 120  # 2 min - engine considered dead
DEFAULT_CHECK_INTERVAL = 30      # Check every 30 sec
MAX_RESTART_ATTEMPTS = 5         # Max restarts before giving up
RESTART_BACKOFF_BASE = 60        # Base backoff: 60, 120, 240, 480, 960 sec
ALERT_COOLDOWN = 300             # Don't spam alerts (5 min cooldown)

# Engine command
PYTHON_EXE = sys.executable
ENGINE_SCRIPT = ROOT / "scripts" / "hope_production_engine.py"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA CLASSES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SupervisorState:
    """Persistent supervisor state."""
    started_at: str = ""
    last_check: str = ""
    engine_restarts: int = 0
    consecutive_failures: int = 0
    last_restart_attempt: str = ""
    last_alert_sent: str = ""
    engine_status: str = "UNKNOWN"  # RUNNING, STOPPED, CRASHED, RESTARTING
    mode: str = "TESTNET"

    def to_dict(self) -> dict:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: dict) -> "SupervisorState":
        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})


@dataclass
class CrashEvent:
    """Crash event for forensics."""
    timestamp: str
    reason: str
    last_heartbeat_age: float
    last_cycle: int
    last_session: str
    positions_open: int
    restart_attempt: int
    action_taken: str  # RESTART, ALERT_ONLY, GIVE_UP

    def to_dict(self) -> dict:
        return asdict(self)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TELEGRAM INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TelegramNotifier:
    """Send alerts via Telegram."""

    def __init__(self):
        self.token = None
        self.admin_id = None
        self._load_config()

    def _load_config(self):
        """Load Telegram config from env."""
        try:
            from dotenv import load_dotenv
            load_dotenv(Path("C:/secrets/hope.env"))
        except ImportError:
            pass

        self.token = os.getenv("TELEGRAM_BOT_TOKEN")
        self.admin_id = os.getenv("HOPE_ADMIN_ID") or os.getenv("TG_ADMIN_ID")

        if self.token and self.admin_id:
            logger.info("Telegram notifications enabled")
        else:
            logger.warning("Telegram not configured - alerts disabled")

    async def send(self, message: str, parse_mode: str = "HTML") -> bool:
        """Send message to admin."""
        if not self.token or not self.admin_id:
            return False

        try:
            import aiohttp
            url = f"https://api.telegram.org/bot{self.token}/sendMessage"
            payload = {
                "chat_id": int(self.admin_id),
                "text": message,
                "parse_mode": parse_mode
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload, timeout=10) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.error(f"Telegram send failed: {e}")
            return False

    async def alert_crash(self, crash: CrashEvent) -> bool:
        """Send crash alert."""
        msg = (
            "ğŸš¨ <b>HOPE ENGINE CRASHED</b>\n\n"
            f"<b>ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:</b> {crash.reason}\n"
            f"<b>ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ heartbeat:</b> {crash.last_heartbeat_age:.0f}s ago\n"
            f"<b>ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ†Ğ¸ĞºĞ»:</b> {crash.last_cycle}\n"
            f"<b>Ğ¡ĞµÑÑĞ¸Ñ:</b> {crash.last_session}\n"
            f"<b>ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹:</b> {crash.positions_open}\n\n"
            f"<b>Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ:</b> {crash.action_taken}\n"
            f"<b>ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ñ€ĞµÑÑ‚Ğ°Ñ€Ñ‚Ğ°:</b> #{crash.restart_attempt}"
        )
        return await self.send(msg)

    async def alert_restart_success(self, attempt: int) -> bool:
        """Send restart success alert."""
        msg = (
            "âœ… <b>ENGINE RESTARTED</b>\n\n"
            f"Ğ”Ğ²Ğ¸Ğ¶Ğ¾Ğº ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½.\n"
            f"ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ°: #{attempt}\n"
            f"Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: RUNNING"
        )
        return await self.send(msg)

    async def alert_give_up(self, attempts: int, reason: str) -> bool:
        """Send give-up alert (max retries exceeded)."""
        msg = (
            "ğŸ’€ <b>SUPERVISOR GAVE UP</b>\n\n"
            f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ¿Ğ¾ÑĞ»Ğµ {attempts} Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº.\n"
            f"ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°: {reason}\n\n"
            "âš ï¸ <b>Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾!</b>\n"
            "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ /stack start Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°."
        )
        return await self.send(msg)

    async def alert_status(self, state: SupervisorState) -> bool:
        """Send status update."""
        msg = (
            "ğŸ“Š <b>SUPERVISOR STATUS</b>\n\n"
            f"<b>Engine:</b> {state.engine_status}\n"
            f"<b>Mode:</b> {state.mode}\n"
            f"<b>Restarts:</b> {state.engine_restarts}\n"
            f"<b>Consecutive failures:</b> {state.consecutive_failures}\n"
            f"<b>Running since:</b> {state.started_at[:19]}"
        )
        return await self.send(msg)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HEARTBEAT MONITOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HeartbeatMonitor:
    """Monitor engine heartbeat file."""

    def __init__(self, timeout_sec: int = DEFAULT_HEARTBEAT_TIMEOUT):
        self.timeout_sec = timeout_sec

    def check(self) -> tuple:
        """
        Check heartbeat status.

        Returns: (is_alive, heartbeat_data, age_sec)
        """
        if not HEARTBEAT_FILE.exists():
            return False, None, float('inf')

        try:
            data = json.loads(HEARTBEAT_FILE.read_text(encoding="utf-8"))
            ts = data.get("timestamp_unix", 0)
            age = time.time() - ts

            is_alive = age < self.timeout_sec
            return is_alive, data, age

        except Exception as e:
            logger.warning(f"Failed to read heartbeat: {e}")
            return False, None, float('inf')

    def get_last_info(self) -> dict:
        """Get last known heartbeat info."""
        try:
            if HEARTBEAT_FILE.exists():
                return json.loads(HEARTBEAT_FILE.read_text(encoding="utf-8"))
        except Exception:
            pass
        return {"cycle": 0, "session": "UNKNOWN", "positions": 0}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENGINE CONTROLLER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EngineController:
    """Control the production engine process."""

    def __init__(self, mode: str = "TESTNET", position_size: float = 15.0):
        self.mode = mode
        self.position_size = position_size
        self.process: Optional[subprocess.Popen] = None

    def is_running(self) -> bool:
        """Check if engine process is running."""
        # Check by PID file
        if ENGINE_LOCK_FILE.exists():
            try:
                pid = int(ENGINE_LOCK_FILE.read_text().strip())
                # Windows-specific check
                import ctypes
                kernel32 = ctypes.windll.kernel32
                handle = kernel32.OpenProcess(0x1000, False, pid)  # PROCESS_QUERY_LIMITED_INFORMATION
                if handle:
                    kernel32.CloseHandle(handle)
                    return True
            except Exception:
                pass

        return False

    def start(self) -> tuple:
        """
        Start the engine.

        Returns: (success, pid_or_error)
        """
        if self.is_running():
            return False, "Already running"

        # Clean up stale lock file
        if ENGINE_LOCK_FILE.exists():
            try:
                ENGINE_LOCK_FILE.unlink()
            except Exception:
                pass

        # Start engine process
        cmd = [
            PYTHON_EXE,
            str(ENGINE_SCRIPT),
            "--mode", self.mode,
            "--position-size", str(self.position_size),
            "--force"  # Ignore lock (we cleaned it)
        ]

        try:
            # Create log files
            LOGS_DIR.mkdir(parents=True, exist_ok=True)
            stdout_log = LOGS_DIR / "engine_stdout.log"
            stderr_log = LOGS_DIR / "engine_stderr.log"

            # Write start marker
            with open(stdout_log, "a", encoding="utf-8") as f:
                f.write(f"\n=== ENGINE START {datetime.now().isoformat()} ===\n")
            with open(stderr_log, "a", encoding="utf-8") as f:
                f.write(f"\n=== ENGINE START {datetime.now().isoformat()} ===\n")

            # Open files for subprocess (keep handles open for child process)
            # Using DETACHED_PROCESS on Windows to allow child to survive parent
            stdout_handle = open(stdout_log, "a", encoding="utf-8")
            stderr_handle = open(stderr_log, "a", encoding="utf-8")

            creationflags = 0
            if sys.platform == "win32":
                # DETACHED_PROCESS (0x8) + CREATE_NO_WINDOW (0x08000000)
                creationflags = subprocess.DETACHED_PROCESS | subprocess.CREATE_NO_WINDOW

            self.process = subprocess.Popen(
                cmd,
                cwd=str(ROOT),
                stdout=stdout_handle,
                stderr=stderr_handle,
                creationflags=creationflags,
                start_new_session=True  # Creates new process group
            )

            # Don't close handles - let the child process own them
            # They will be closed when the child exits

            logger.info(f"Engine started with PID {self.process.pid}")
            return True, self.process.pid

        except Exception as e:
            logger.error(f"Failed to start engine: {e}")
            return False, str(e)

    def stop(self) -> bool:
        """Stop the engine gracefully."""
        # Set STOP flag
        STOP_FLAG_FILE.parent.mkdir(parents=True, exist_ok=True)
        STOP_FLAG_FILE.write_text(
            f"STOPPED by supervisor at {datetime.now().isoformat()}",
            encoding="utf-8"
        )

        # Wait for graceful shutdown
        for _ in range(30):  # 30 sec timeout
            if not self.is_running():
                return True
            time.sleep(1)

        # Force kill if still running
        if ENGINE_LOCK_FILE.exists():
            try:
                pid = int(ENGINE_LOCK_FILE.read_text().strip())
                os.kill(pid, signal.SIGTERM)
                ENGINE_LOCK_FILE.unlink()
            except Exception:
                pass

        return not self.is_running()

    def get_pid(self) -> Optional[int]:
        """Get engine PID."""
        if ENGINE_LOCK_FILE.exists():
            try:
                return int(ENGINE_LOCK_FILE.read_text().strip())
            except Exception:
                pass
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CRASH FORENSICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CrashForensics:
    """Analyze and log crash events."""

    @staticmethod
    def analyze(heartbeat_data: Optional[dict], heartbeat_age: float) -> str:
        """Determine probable crash reason."""
        if heartbeat_data is None:
            return "NO_HEARTBEAT_FILE"

        if heartbeat_age > 3600:
            return "ENGINE_NOT_STARTED"

        if heartbeat_age > 300:
            return "ENGINE_HUNG"

        # Check last known state
        positions = heartbeat_data.get("positions", 0)
        cycle = heartbeat_data.get("cycle", 0)

        if positions > 0:
            return "CRASH_WITH_OPEN_POSITIONS"

        if cycle < 10:
            return "EARLY_CRASH"

        return "UNKNOWN_CRASH"

    @staticmethod
    def log_crash(crash: CrashEvent):
        """Log crash event to JSONL."""
        CRASH_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

        with open(CRASH_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(crash.to_dict()) + "\n")

        logger.warning(f"Crash logged: {crash.reason}")

    @staticmethod
    def get_recent_crashes(limit: int = 10) -> List[dict]:
        """Get recent crash events."""
        if not CRASH_LOG_FILE.exists():
            return []

        crashes = []
        try:
            lines = CRASH_LOG_FILE.read_text(encoding="utf-8").strip().split("\n")
            for line in lines[-limit:]:
                if line.strip():
                    crashes.append(json.loads(line))
        except Exception:
            pass

        return crashes


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI SUPERVISOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AISupervisor:
    """
    AI Supervisor - Watchdog daemon for Production Engine.

    Monitors heartbeat, auto-restarts on crash, sends Telegram alerts.
    """

    def __init__(
        self,
        mode: str = "TESTNET",
        check_interval: int = DEFAULT_CHECK_INTERVAL,
        heartbeat_timeout: int = DEFAULT_HEARTBEAT_TIMEOUT,
    ):
        self.mode = mode
        self.check_interval = check_interval
        self.running = False

        # Components
        self.monitor = HeartbeatMonitor(heartbeat_timeout)
        self.controller = EngineController(mode)
        self.telegram = TelegramNotifier()
        self.forensics = CrashForensics()

        # State
        self.state = SupervisorState(
            started_at=datetime.now(timezone.utc).isoformat(),
            mode=mode
        )

        # Load previous state if exists
        self._load_state()

    def _load_state(self):
        """Load state from file."""
        if SUPERVISOR_STATE_FILE.exists():
            try:
                data = json.loads(SUPERVISOR_STATE_FILE.read_text(encoding="utf-8"))
                # Only restore counters, not timestamps
                self.state.engine_restarts = data.get("engine_restarts", 0)
            except Exception:
                pass

    def _save_state(self):
        """Save state to file."""
        SUPERVISOR_STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        SUPERVISOR_STATE_FILE.write_text(
            json.dumps(self.state.to_dict(), indent=2),
            encoding="utf-8"
        )

    def _should_alert(self) -> bool:
        """Check if we should send an alert (cooldown)."""
        if not self.state.last_alert_sent:
            return True

        try:
            last = datetime.fromisoformat(self.state.last_alert_sent.replace("Z", "+00:00"))
            elapsed = (datetime.now(timezone.utc) - last).total_seconds()
            return elapsed > ALERT_COOLDOWN
        except Exception:
            return True

    def _calculate_backoff(self) -> int:
        """Calculate restart backoff time."""
        failures = min(self.state.consecutive_failures, MAX_RESTART_ATTEMPTS)
        return RESTART_BACKOFF_BASE * (2 ** failures)

    async def _handle_crash(self, heartbeat_data: Optional[dict], heartbeat_age: float):
        """Handle detected crash."""
        # Analyze crash
        reason = self.forensics.analyze(heartbeat_data, heartbeat_age)
        last_info = self.monitor.get_last_info()

        self.state.consecutive_failures += 1
        self.state.engine_status = "CRASHED"

        # Determine action
        if self.state.consecutive_failures > MAX_RESTART_ATTEMPTS:
            action = "GIVE_UP"
        else:
            action = "RESTART"

        # Log crash
        crash = CrashEvent(
            timestamp=datetime.now(timezone.utc).isoformat(),
            reason=reason,
            last_heartbeat_age=heartbeat_age,
            last_cycle=last_info.get("cycle", 0),
            last_session=last_info.get("session", "UNKNOWN"),
            positions_open=last_info.get("positions", 0),
            restart_attempt=self.state.consecutive_failures,
            action_taken=action
        )
        self.forensics.log_crash(crash)

        # Send alert
        if self._should_alert():
            await self.telegram.alert_crash(crash)
            self.state.last_alert_sent = datetime.now(timezone.utc).isoformat()

        # Take action
        if action == "RESTART":
            await self._attempt_restart()
        elif action == "GIVE_UP":
            logger.error(f"Giving up after {MAX_RESTART_ATTEMPTS} attempts")
            await self.telegram.alert_give_up(MAX_RESTART_ATTEMPTS, reason)
            self.state.engine_status = "DEAD"

    async def _attempt_restart(self):
        """Attempt to restart the engine."""
        # Wait backoff time
        backoff = self._calculate_backoff()
        logger.info(f"Waiting {backoff}s before restart (attempt {self.state.consecutive_failures})")

        self.state.engine_status = "RESTARTING"
        self.state.last_restart_attempt = datetime.now(timezone.utc).isoformat()
        self._save_state()

        await asyncio.sleep(backoff)

        # Check STOP flag
        if STOP_FLAG_FILE.exists():
            logger.info("STOP flag set - not restarting")
            return

        # Start engine
        success, result = self.controller.start()

        if success:
            logger.info(f"Engine restarted successfully (PID {result})")
            self.state.engine_restarts += 1
            self.state.engine_status = "RUNNING"
            # Don't reset consecutive_failures yet - wait for stable heartbeat

            # Wait for heartbeat
            await asyncio.sleep(30)
            is_alive, _, _ = self.monitor.check()

            if is_alive:
                logger.info("Engine heartbeat confirmed")
                self.state.consecutive_failures = 0  # Reset on confirmed success
                await self.telegram.alert_restart_success(self.state.engine_restarts)
            else:
                logger.warning("Engine started but no heartbeat")
        else:
            logger.error(f"Failed to start engine: {result}")
            self.state.engine_status = "CRASHED"

    async def run(self):
        """Main supervisor loop."""
        logger.info(f"AI Supervisor started (mode={self.mode}, interval={self.check_interval}s)")
        self.running = True

        # Acquire lock
        LOCKS_DIR.mkdir(parents=True, exist_ok=True)
        SUPERVISOR_LOCK_FILE.write_text(str(os.getpid()), encoding="utf-8")

        try:
            while self.running:
                # Check STOP flag
                if STOP_FLAG_FILE.exists():
                    logger.info("STOP flag detected")
                    break

                # Check heartbeat
                is_alive, heartbeat_data, age = self.monitor.check()
                self.state.last_check = datetime.now(timezone.utc).isoformat()

                if is_alive:
                    self.state.engine_status = "RUNNING"
                    if heartbeat_data:
                        cycle = heartbeat_data.get("cycle", 0)
                        session = heartbeat_data.get("session", "?")
                        positions = heartbeat_data.get("positions", 0)
                        logger.debug(f"Engine OK | cycle={cycle} | session={session} | pos={positions}")
                else:
                    # Engine crashed or not running
                    if self.state.engine_status != "DEAD":
                        logger.warning(f"Engine not responding (age={age:.0f}s)")
                        await self._handle_crash(heartbeat_data, age)

                self._save_state()
                await asyncio.sleep(self.check_interval)

        except KeyboardInterrupt:
            logger.info("Shutdown requested")
        finally:
            self.running = False
            # Release lock
            try:
                SUPERVISOR_LOCK_FILE.unlink()
            except Exception:
                pass
            logger.info("Supervisor stopped")

    def get_status(self) -> dict:
        """Get supervisor status."""
        is_alive, heartbeat_data, age = self.monitor.check()

        return {
            "supervisor": {
                "running": self.running,
                "started_at": self.state.started_at,
                "mode": self.state.mode,
            },
            "engine": {
                "status": self.state.engine_status,
                "pid": self.controller.get_pid(),
                "is_alive": is_alive,
                "heartbeat_age": age if age != float('inf') else None,
                "last_cycle": heartbeat_data.get("cycle") if heartbeat_data else None,
                "last_session": heartbeat_data.get("session") if heartbeat_data else None,
            },
            "stats": {
                "total_restarts": self.state.engine_restarts,
                "consecutive_failures": self.state.consecutive_failures,
                "last_restart": self.state.last_restart_attempt,
            }
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    parser = argparse.ArgumentParser(description="HOPE AI Supervisor")
    parser.add_argument("--mode", type=str, default="TESTNET",
                       choices=["DRY", "TESTNET", "LIVE"])
    parser.add_argument("--interval", type=int, default=DEFAULT_CHECK_INTERVAL,
                       help=f"Check interval in seconds (default: {DEFAULT_CHECK_INTERVAL})")
    parser.add_argument("--timeout", type=int, default=DEFAULT_HEARTBEAT_TIMEOUT,
                       help=f"Heartbeat timeout in seconds (default: {DEFAULT_HEARTBEAT_TIMEOUT})")
    parser.add_argument("--status", action="store_true", help="Show status and exit")
    parser.add_argument("--stop", action="store_true", help="Stop supervisor and engine")
    parser.add_argument("--start-engine", action="store_true", help="Start engine only")
    args = parser.parse_args()

    # Load env
    try:
        from dotenv import load_dotenv
        load_dotenv(Path("C:/secrets/hope.env"))
    except ImportError:
        pass

    supervisor = AISupervisor(
        mode=args.mode,
        check_interval=args.interval,
        heartbeat_timeout=args.timeout
    )

    if args.status:
        status = supervisor.get_status()
        print(json.dumps(status, indent=2, default=str))
        return

    if args.stop:
        # Set STOP flag
        STOP_FLAG_FILE.write_text(
            f"STOPPED by user at {datetime.now().isoformat()}",
            encoding="utf-8"
        )
        print("STOP flag set. Supervisor and engine will stop.")
        return

    if args.start_engine:
        success, result = supervisor.controller.start()
        if success:
            print(f"Engine started (PID: {result})")
        else:
            print(f"Failed to start engine: {result}")
        return

    # Check single instance
    if SUPERVISOR_LOCK_FILE.exists():
        try:
            pid = int(SUPERVISOR_LOCK_FILE.read_text().strip())
            import ctypes
            kernel32 = ctypes.windll.kernel32
            handle = kernel32.OpenProcess(0x1000, False, pid)
            if handle:
                kernel32.CloseHandle(handle)
                print(f"Supervisor already running (PID {pid})")
                sys.exit(1)
        except Exception:
            pass

    # Remove STOP flag if exists
    if STOP_FLAG_FILE.exists():
        STOP_FLAG_FILE.unlink()

    # Run supervisor
    await supervisor.run()


if __name__ == "__main__":
    asyncio.run(main())
